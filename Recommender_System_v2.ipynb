{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install pandas\n",
        "! pip install numpy\n",
        "! pip install scikit-learn\n",
        "! pip install matplotlib\n",
        "! pip install joblib\n",
        "! pip install streamlit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNLBumsrB1uH",
        "outputId": "e8be1c6a-8ade-42f1-a42a-36ae50b9de73"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.50.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.7.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.50.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.50.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNBcw5LQBzGM",
        "outputId": "98327323-b587-4a80-cb17-47578aa0954d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Building dataset...\n",
            ">> Train/test split...\n",
            ">> Training CF (TruncatedSVD)...\n",
            "CF trained. Pred matrix shape: (600, 350)\n",
            ">> Training CBF (TF-IDF)...\n",
            ">> Evaluation (RMSE + Precision@5)...\n",
            "RMSE (approx): 0.9173, missing preds: 0\n",
            "Precision@5 (CF approx): 0.0045\n",
            "\n",
            "Top 5 hybrid recs for U378:\n",
            "- P12 | Books Product 12 | score=0.5282\n",
            "   • Users with tastes like yours rated this highly.\n",
            "   • Similar to what you liked: 'Books Product 198'.\n",
            "- P48 | Books Product 48 | score=0.5261\n",
            "   • Users with tastes like yours rated this highly.\n",
            "   • Similar to what you liked: 'Books Product 149'.\n",
            "- P44 | Books Product 44 | score=0.5235\n",
            "   • Users with tastes like yours rated this highly.\n",
            "   • Similar to what you liked: 'Books Product 99'.\n",
            "- P126 | Books Product 126 | score=0.5212\n",
            "   • Users with tastes like yours rated this highly.\n",
            "   • Similar to what you liked: 'Books Product 99'.\n",
            "- P152 | Books Product 152 | score=0.5150\n",
            "   • Users with tastes like yours rated this highly.\n",
            "   • Similar to what you liked: 'Books Product 99'.\n",
            "\n",
            "Top 5 hybrid recs for U393:\n",
            "- P252 | Beauty Product 252 | score=0.5560\n",
            "   • Users with tastes like yours rated this highly.\n",
            "   • Similar to what you liked: 'Beauty Product 86'.\n",
            "- P20 | Beauty Product 20 | score=0.5377\n",
            "   • Users with tastes like yours rated this highly.\n",
            "   • Similar to what you liked: 'Beauty Product 86'.\n",
            "- P245 | Beauty Product 245 | score=0.5376\n",
            "   • Users with tastes like yours rated this highly.\n",
            "   • Similar to what you liked: 'Beauty Product 86'.\n",
            "- P290 | Beauty Product 290 | score=0.5012\n",
            "   • Users with tastes like yours rated this highly.\n",
            "   • Similar to what you liked: 'Beauty Product 109'.\n",
            "- P295 | Beauty Product 295 | score=0.5000\n",
            "   • Users with tastes like yours rated this highly.\n",
            "   • Similar to what you liked: 'Beauty Product 109'.\n",
            "\n",
            "Top 5 hybrid recs for U445:\n",
            "- P327 | Toys Product 327 | score=0.5811\n",
            "   • Users with tastes like yours rated this highly.\n",
            "   • Similar to what you liked: 'Toys Product 78'.\n",
            "- P4 | Toys Product 4 | score=0.5743\n",
            "   • Users with tastes like yours rated this highly.\n",
            "   • Similar to what you liked: 'Toys Product 204'.\n",
            "- P240 | Toys Product 240 | score=0.5626\n",
            "   • Users with tastes like yours rated this highly.\n",
            "   • Similar to what you liked: 'Toys Product 204'.\n",
            "- P281 | Toys Product 281 | score=0.5537\n",
            "   • Users with tastes like yours rated this highly.\n",
            "   • Similar to what you liked: 'Toys Product 197'.\n",
            "- P67 | Toys Product 67 | score=0.5533\n",
            "   • Users with tastes like yours rated this highly.\n",
            "   • Similar to what you liked: 'Toys Product 78'.\n",
            "\n",
            "Saved artifacts under recommender_app/models\n",
            "\n",
            "Interactive example for user: U378\n",
            "- P12 | Books Product 12 | score=0.5282 -- reasons: ['Users with tastes like yours rated this highly.', \"Similar to what you liked: 'Books Product 198'.\"]\n",
            "- P48 | Books Product 48 | score=0.5261 -- reasons: ['Users with tastes like yours rated this highly.', \"Similar to what you liked: 'Books Product 149'.\"]\n",
            "- P44 | Books Product 44 | score=0.5235 -- reasons: ['Users with tastes like yours rated this highly.', \"Similar to what you liked: 'Books Product 99'.\"]\n",
            "- P126 | Books Product 126 | score=0.5212 -- reasons: ['Users with tastes like yours rated this highly.', \"Similar to what you liked: 'Books Product 99'.\"]\n",
            "- P152 | Books Product 152 | score=0.5150 -- reasons: ['Users with tastes like yours rated this highly.', \"Similar to what you liked: 'Books Product 99'.\"]\n",
            "- P7 | Books Product 7 | score=0.5145 -- reasons: ['Users with tastes like yours rated this highly.', \"Similar to what you liked: 'Books Product 99'.\", 'Popular (top 9 most-rated items).']\n",
            "- P75 | Books Product 75 | score=0.5132 -- reasons: ['Users with tastes like yours rated this highly.', \"Similar to what you liked: 'Books Product 198'.\"]\n"
          ]
        }
      ],
      "source": [
        "# smart_recommender.py\n",
        "\"\"\"\n",
        "Smart Hybrid Recommender with Weighted Hybrid, Implicit Feedback, and Explainability\n",
        "Author: Harshit Mittal (adapted)\n",
        "Date: 2025-10-11\n",
        "Run: python smart_recommender.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 1) Utilities & Dataset\n",
        "# -------------------------\n",
        "def build_synthetic_dataset(n_users=600, n_items=350, min_ratings_per_user=10, max_ratings_per_user=25):\n",
        "    \"\"\"\n",
        "    Build a synthetic dataset with userId, productId, rating and product metadata.\n",
        "    Returns: ratings_df (userId, productId, rating), products_df (productId, title, category, description)\n",
        "    \"\"\"\n",
        "    users = [f\"U{u}\" for u in range(1, n_users + 1)]\n",
        "    items = [f\"P{i}\" for i in range(1, n_items + 1)]\n",
        "    categories = ['Electronics', 'Books', 'Home', 'Toys', 'Beauty', 'Sports', 'Clothing', 'Grocery']\n",
        "\n",
        "    rows = []\n",
        "    for u in users:\n",
        "        k = np.random.randint(min_ratings_per_user, max_ratings_per_user + 1)\n",
        "        sampled = np.random.choice(items, size=k, replace=False)\n",
        "        for it in sampled:\n",
        "            # realistic-ish distribution: normal around 3.6, clipped to [1,5], rounded to 0.5 steps\n",
        "            r = np.random.normal(loc=3.6, scale=1.0)\n",
        "            r = min(5.0, max(1.0, r))\n",
        "            r = round(r * 2) / 2.0\n",
        "            rows.append((u, it, r))\n",
        "    ratings_df = pd.DataFrame(rows, columns=['userId', 'productId', 'rating'])\n",
        "\n",
        "    # product metadata\n",
        "    product_meta = []\n",
        "    keywords = {\n",
        "        'Electronics': ['battery', 'wireless', 'bluetooth', 'USB', 'portable', 'charger'],\n",
        "        'Books': ['story', 'novel', 'guide', 'history', 'author', 'learn'],\n",
        "        'Home': ['kitchen', 'durable', 'design', 'compact', 'decor', 'clean'],\n",
        "        'Toys': ['kids', 'fun', 'safe', 'interactive', 'educational', 'colorful'],\n",
        "        'Beauty': ['gentle', 'skin', 'organic', 'scent', 'serum', 'moisturizer'],\n",
        "        'Sports': ['fitness', 'outdoor', 'durable', 'training', 'performance', 'comfort'],\n",
        "        'Clothing': ['fabric', 'comfortable', 'casual', 'size', 'style', 'soft'],\n",
        "        'Grocery': ['fresh', 'organic', 'snack', 'ingredients', 'package', 'tasty']\n",
        "    }\n",
        "    for i, pid in enumerate(items, start=1):\n",
        "        cat = random.choice(categories)\n",
        "        title = f\"{cat} Product {i}\"\n",
        "        desc_words = \" \".join(np.random.choice(keywords[cat], size=6, replace=True))\n",
        "        description = f\"{title}. {desc_words}. High quality and good value.\"\n",
        "        product_meta.append({'productId': pid, 'title': title, 'category': cat, 'description': description})\n",
        "    products_df = pd.DataFrame(product_meta)\n",
        "    return ratings_df, products_df\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 2) Preprocess & EDA (brief)\n",
        "# -------------------------\n",
        "def preprocess(ratings_df):\n",
        "    ratings_df = ratings_df.drop_duplicates().dropna(subset=['userId', 'productId', 'rating']).reset_index(drop=True)\n",
        "    ratings_df['userId'] = ratings_df['userId'].astype(str)\n",
        "    ratings_df['productId'] = ratings_df['productId'].astype(str)\n",
        "    ratings_df['rating'] = pd.to_numeric(ratings_df['rating'], errors='coerce').astype(float)\n",
        "    return ratings_df\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 3) Simulate Implicit Feedback\n",
        "# -------------------------\n",
        "def add_implicit_scores(ratings_df, click_multiplier=(0.8, 1.4)):\n",
        "    \"\"\"\n",
        "    Create an 'implicit_score' column as a function of explicit rating and a random click/view factor.\n",
        "    This mimics implicit feedback (clicks, views).\n",
        "    \"\"\"\n",
        "    mults = np.random.uniform(click_multiplier[0], click_multiplier[1], size=len(ratings_df))\n",
        "    ratings_df = ratings_df.copy()\n",
        "    ratings_df['implicit_score'] = (ratings_df['rating'] / 5.0) * mults  # normalized\n",
        "    return ratings_df\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 4) Collaborative Filtering (matrix factorization) - offline approach\n",
        "# -------------------------\n",
        "def train_cf(train_df, n_components=30):\n",
        "    \"\"\"\n",
        "    Train TruncatedSVD on the user-item matrix (train only).\n",
        "    Returns:\n",
        "      - svd (TruncatedSVD fitted on filled matrix)\n",
        "      - user_index (list of user ids), item_index (list of item ids)\n",
        "      - pred_matrix_df: DataFrame of predicted ratings (users x items)\n",
        "    \"\"\"\n",
        "    # create pivot (users x items) with NaNs for unrated\n",
        "    user_ids = sorted(train_df['userId'].unique())\n",
        "    item_ids = sorted(train_df['productId'].unique())\n",
        "    pivot = train_df.pivot_table(index='userId', columns='productId', values='rating')\n",
        "    pivot = pivot.reindex(index=user_ids, columns=item_ids)\n",
        "\n",
        "    # fill missing with user mean (or global mean)\n",
        "    filled = pivot.copy()\n",
        "    user_mean = pivot.mean(axis=1)\n",
        "    global_mean = train_df['rating'].mean()\n",
        "    for u in user_ids:\n",
        "        if not np.isnan(user_mean.loc[u]):\n",
        "            filled.loc[u] = filled.loc[u].fillna(user_mean.loc[u])\n",
        "        else:\n",
        "            filled.loc[u] = filled.loc[u].fillna(global_mean)\n",
        "\n",
        "    svd = TruncatedSVD(n_components=n_components, random_state=SEED)\n",
        "    user_factors = svd.fit_transform(filled.values)  # users x components\n",
        "    item_factors = svd.components_.T  # items x components\n",
        "\n",
        "    reconstructed = np.dot(user_factors, item_factors.T)\n",
        "    pred_matrix_df = pd.DataFrame(reconstructed, index=user_ids, columns=item_ids)\n",
        "\n",
        "    return svd, user_ids, item_ids, pred_matrix_df, filled\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 5) Content-Based Filtering (TF-IDF on product descriptions)\n",
        "# -------------------------\n",
        "def train_cbf(products_df, max_features=2000):\n",
        "    \"\"\"\n",
        "    Train TF-IDF on product descriptions and return tfidf vectorizer and matrix (sparse).\n",
        "    \"\"\"\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english')\n",
        "    descs = products_df['description'].fillna(products_df['title'].fillna(''))\n",
        "    tfidf_matrix = vectorizer.fit_transform(descs)\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 6) Scoring & Weighted Hybrid\n",
        "# -------------------------\n",
        "def compute_weighted_score(user_id, product_id, pred_matrix_df, tfidf_matrix, products_df,\n",
        "                           rating_history_count, alpha_cf, alpha_cbf, implicit_weight=0.2,\n",
        "                           ratings_df=None):\n",
        "    \"\"\"\n",
        "    Compute final score for a user-product combining:\n",
        "      - collaborative estimate (from pred_matrix_df)\n",
        "      - content similarity score (if user has liked items)\n",
        "      - implicit feedback adjustment (if available)\n",
        "    alpha_cf + alpha_cbf should be 1.0 (weights for CF and CBF).\n",
        "    implicit_weight is additional multiplier for implicit signals.\n",
        "    \"\"\"\n",
        "    # collaborative estimate (if available)\n",
        "    try:\n",
        "        cf_score = pred_matrix_df.loc[user_id, product_id]\n",
        "    except Exception:\n",
        "        cf_score = None\n",
        "\n",
        "    # content-based: average similarity between the product and user's liked items (rating>=4)\n",
        "    cbf_score = 0.0\n",
        "    if ratings_df is not None:\n",
        "        liked = ratings_df[(ratings_df['userId'] == user_id) & (ratings_df['rating'] >= 4.0)]['productId'].tolist()\n",
        "    else:\n",
        "        liked = []\n",
        "    if len(liked) > 0:\n",
        "        # map product ids to indices in products_df\n",
        "        prod_to_idx = {pid: idx for idx, pid in enumerate(products_df['productId'])}\n",
        "        if product_id not in prod_to_idx:\n",
        "            cbf_score = 0.0\n",
        "        else:\n",
        "            p_idx = prod_to_idx[product_id]\n",
        "            liked_idxs = [prod_to_idx[p] for p in liked if p in prod_to_idx]\n",
        "            if len(liked_idxs) == 0:\n",
        "                cbf_score = 0.0\n",
        "            else:\n",
        "                # cosine similarity average\n",
        "                sims = cosine_similarity(tfidf_matrix[p_idx], tfidf_matrix[liked_idxs]).flatten()\n",
        "                cbf_score = float(np.mean(sims))\n",
        "    else:\n",
        "        # cold-start: no liked items -> cbf_score can be similarity to popular items; keep 0 to let global popularity fill\n",
        "        cbf_score = 0.0\n",
        "\n",
        "    # Normalize cf_score to 0-1 based on rating range if present (we assume approx 1-5 ratings)\n",
        "    if cf_score is None:\n",
        "        cf_norm = 0.0\n",
        "    else:\n",
        "        cf_norm = (cf_score - 1.0) / 4.0  # maps 1..5 -> 0..1\n",
        "\n",
        "    # final weighted score\n",
        "    score = alpha_cf * cf_norm + alpha_cbf * cbf_score\n",
        "\n",
        "    # implicit feedback boost: if the user previously had high implicit scores for similar items, boost\n",
        "    if ratings_df is not None and implicit_weight > 0:\n",
        "        # compute mean implicit_score for user's liked items (>=4)\n",
        "        user_implicit_mean = ratings_df[(ratings_df['userId'] == user_id) & (ratings_df['rating'] >= 4.0)]['implicit_score'].mean()\n",
        "        if not np.isnan(user_implicit_mean):\n",
        "            score = score * (1.0 + implicit_weight * user_implicit_mean)\n",
        "\n",
        "    return float(score), float(cf_norm), float(cbf_score)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 7) Dynamic weight function\n",
        "# -------------------------\n",
        "def dynamic_weights(num_user_ratings, min_ratings=5, max_ratings=40):\n",
        "    \"\"\"\n",
        "    Decide alpha_cf and alpha_cbf dynamically.\n",
        "    If user has many ratings -> rely more on CF.\n",
        "    If user has few ratings -> rely more on CBF.\n",
        "    Returns (alpha_cf, alpha_cbf)\n",
        "    \"\"\"\n",
        "    # map num_user_ratings in [0, max_ratings] to alpha_cf in [0.2, 0.9]\n",
        "    if num_user_ratings <= min_ratings:\n",
        "        alpha_cf = 0.2\n",
        "    else:\n",
        "        # linear mapping\n",
        "        alpha_cf = 0.2 + 0.7 * min(num_user_ratings - min_ratings, max_ratings - min_ratings) / (max_ratings - min_ratings)\n",
        "    alpha_cbf = 1.0 - alpha_cf\n",
        "    return alpha_cf, alpha_cbf\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 8) Recommendation function with explanations\n",
        "# -------------------------\n",
        "def recommend_products(user_id, n, pred_matrix_df, tfidf_matrix, products_df, ratings_df,\n",
        "                       method='hybrid', implicit_weight=0.2):\n",
        "    \"\"\"\n",
        "    Return top-n product recommendations for user_id along with short explanations.\n",
        "    method: 'cf', 'cbf', or 'hybrid'\n",
        "    Output: list of dicts: {productId, title, score, reason}\n",
        "    \"\"\"\n",
        "    all_products = products_df['productId'].tolist()\n",
        "    # compute how many ratings the user has in training data\n",
        "    num_user_ratings = len(ratings_df[ratings_df['userId'] == user_id])\n",
        "    alpha_cf, alpha_cbf = dynamic_weights(num_user_ratings)\n",
        "\n",
        "    # fallback popular items if user unknown\n",
        "    user_known = (user_id in pred_matrix_df.index)\n",
        "    popular = list(ratings_df['productId'].value_counts().index)\n",
        "\n",
        "    candidates = [p for p in all_products if p not in ratings_df[ratings_df['userId'] == user_id]['productId'].tolist()]\n",
        "\n",
        "    scored = []\n",
        "    for p in candidates:\n",
        "        if method == 'cf':\n",
        "            alpha_cf_use, alpha_cbf_use = 1.0, 0.0\n",
        "        elif method == 'cbf':\n",
        "            alpha_cf_use, alpha_cbf_use = 0.0, 1.0\n",
        "        else:  # hybrid\n",
        "            alpha_cf_use, alpha_cbf_use = alpha_cf, alpha_cbf\n",
        "\n",
        "        score, cf_norm, cbf_score = compute_weighted_score(\n",
        "            user_id, p, pred_matrix_df, tfidf_matrix, products_df,\n",
        "            rating_history_count=num_user_ratings,\n",
        "            alpha_cf=alpha_cf_use, alpha_cbf=alpha_cbf_use,\n",
        "            implicit_weight=implicit_weight, ratings_df=ratings_df\n",
        "        )\n",
        "\n",
        "        scored.append((p, score, cf_norm, cbf_score))\n",
        "\n",
        "    # sort by score\n",
        "    scored_sorted = sorted(scored, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # build explanation for each top item\n",
        "    prod_to_meta = {row['productId']: row for _, row in products_df.iterrows()}\n",
        "    recommendations = []\n",
        "    for pid, score, cf_norm, cbf_score in scored_sorted[:n]:\n",
        "        meta = prod_to_meta.get(pid, {})\n",
        "        reasons = []\n",
        "        # reason templates\n",
        "        if cf_norm > 0.45:\n",
        "            # collaborative is strong\n",
        "            reasons.append(\"Users with tastes like yours rated this highly.\")\n",
        "        if cbf_score > 0.15:\n",
        "            # content similarity strong (threshold tunable)\n",
        "            # find the most similar liked product name to mention\n",
        "            liked = ratings_df[(ratings_df['userId'] == user_id) & (ratings_df['rating'] >= 4.0)]['productId'].tolist()\n",
        "            if len(liked) > 0:\n",
        "                prod_to_idx = {prod: idx for idx, prod in enumerate(products_df['productId'])}\n",
        "                if pid in prod_to_idx:\n",
        "                    p_idx = prod_to_idx[pid]\n",
        "                    liked_idxs = [prod_to_idx[l] for l in liked if l in prod_to_idx]\n",
        "                    if len(liked_idxs) > 0:\n",
        "                        sims = cosine_similarity(tfidf_matrix[p_idx], tfidf_matrix[liked_idxs]).flatten()\n",
        "                        best_idx = liked_idxs[int(np.argmax(sims))]\n",
        "                        best_prod = products_df.iloc[best_idx]['title']\n",
        "                        reasons.append(f\"Similar to what you liked: '{best_prod}'.\")\n",
        "            else:\n",
        "                reasons.append(\"Has features similar to products you might like.\")\n",
        "        # popularity reason\n",
        "        pop_rank = popular.index(pid) + 1 if pid in popular else None\n",
        "        if pop_rank and pop_rank <= 20:\n",
        "            reasons.append(f\"Popular (top {min(pop_rank,20)} most-rated items).\")\n",
        "        if len(reasons) == 0:\n",
        "            reasons.append(\"Recommended based on hybrid scoring.\")\n",
        "\n",
        "        recommendations.append({\n",
        "            'productId': pid,\n",
        "            'title': meta.get('title', ''),\n",
        "            'category': meta.get('category', ''),\n",
        "            'score': float(score),\n",
        "            'reasons': reasons\n",
        "        })\n",
        "\n",
        "    # If no candidates or user unknown, fallback to top popular\n",
        "    if len(recommendations) == 0:\n",
        "        for pid in popular[:n]:\n",
        "            meta = prod_to_meta.get(pid, {})\n",
        "            recommendations.append({\n",
        "                'productId': pid,\n",
        "                'title': meta.get('title', ''),\n",
        "                'category': meta.get('category', ''),\n",
        "                'score': None,\n",
        "                'reasons': [\"Fallback: popular item.\"]\n",
        "            })\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 9) Evaluation functions\n",
        "# -------------------------\n",
        "def evaluate_rmse(pred_matrix_df, test_df):\n",
        "    y_true, y_pred = [], []\n",
        "    missing = 0\n",
        "    for _, row in test_df.iterrows():\n",
        "        u, p, r = row['userId'], row['productId'], row['rating']\n",
        "        if (u in pred_matrix_df.index) and (p in pred_matrix_df.columns):\n",
        "            y_true.append(r)\n",
        "            y_pred.append(pred_matrix_df.loc[u, p])\n",
        "        else:\n",
        "            missing += 1\n",
        "    rmse = mean_squared_error(y_true, y_pred) if len(y_true) > 0 else None\n",
        "    return rmse, missing\n",
        "\n",
        "\n",
        "def precision_at_k(pred_matrix_df, train_df, test_df, k=5, threshold=4.0):\n",
        "    users = test_df['userId'].unique()\n",
        "    precisions = []\n",
        "    for u in users:\n",
        "        if u not in pred_matrix_df.index:\n",
        "            continue\n",
        "        train_items = set(train_df[train_df['userId'] == u]['productId'].tolist())\n",
        "        all_items = set(pred_matrix_df.columns)\n",
        "        candidates = list(all_items - train_items)\n",
        "        if len(candidates) == 0:\n",
        "            continue\n",
        "        scores = [(iid, pred_matrix_df.loc[u, iid]) for iid in candidates]\n",
        "        scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        topk = [iid for iid, _ in scores[:k]]\n",
        "        relevant = set(test_df[(test_df['userId'] == u) & (test_df['rating'] >= threshold)]['productId'].tolist())\n",
        "        if len(topk) == 0:\n",
        "            continue\n",
        "        prec = len([i for i in topk if i in relevant]) / len(topk)\n",
        "        precisions.append(prec)\n",
        "    return np.mean(precisions) if len(precisions) > 0 else 0.0\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 10) Orchestration: train everything and demo\n",
        "# -------------------------\n",
        "def train_and_demo(save_artifacts=True):\n",
        "    print(\">> Building dataset...\")\n",
        "    ratings_df, products_df = build_synthetic_dataset()\n",
        "    ratings_df = preprocess(ratings_df)\n",
        "    ratings_df = add_implicit_scores(ratings_df)\n",
        "\n",
        "    print(\">> Train/test split...\")\n",
        "    train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=SEED)\n",
        "\n",
        "    print(\">> Training CF (TruncatedSVD)...\")\n",
        "    svd, user_ids, item_ids, pred_matrix_df, filled_matrix = train_cf(train_df, n_components=30)\n",
        "    print(\"CF trained. Pred matrix shape:\", pred_matrix_df.shape)\n",
        "\n",
        "    print(\">> Training CBF (TF-IDF)...\")\n",
        "    tfidf_vectorizer, tfidf_matrix = train_cbf(products_df)\n",
        "\n",
        "    print(\">> Evaluation (RMSE + Precision@5)...\")\n",
        "    rmse, missing = evaluate_rmse(pred_matrix_df, test_df)\n",
        "    p5 = precision_at_k(pred_matrix_df, train_df, test_df, k=5, threshold=4.0)\n",
        "    print(f\"RMSE (approx): {rmse:.4f}, missing preds: {missing}\")\n",
        "    print(f\"Precision@5 (CF approx): {p5:.4f}\")\n",
        "\n",
        "    # Quick demo: show recommendations for three users\n",
        "    sample_users = train_df['userId'].unique()[:3]\n",
        "    for u in sample_users:\n",
        "        recs = recommend_products(u, n=5, pred_matrix_df=pred_matrix_df,\n",
        "                                  tfidf_matrix=tfidf_matrix, products_df=products_df,\n",
        "                                  ratings_df=train_df, method='hybrid')\n",
        "        print(f\"\\nTop 5 hybrid recs for {u}:\")\n",
        "        for r in recs:\n",
        "            print(f\"- {r['productId']} | {r['title'][:40]} | score={r['score']:.4f}\")\n",
        "            for reason in r['reasons']:\n",
        "                print(\"   •\", reason)\n",
        "\n",
        "    # Optionally save artifacts for Streamlit app\n",
        "    if save_artifacts:\n",
        "        out_dir = \"recommender_app/models\"\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        joblib.dump(pred_matrix_df, os.path.join(out_dir, \"pred_matrix_df.pkl\"))\n",
        "        joblib.dump(svd, os.path.join(out_dir, \"truncated_svd.pkl\"))\n",
        "        joblib.dump(tfidf_vectorizer, os.path.join(out_dir, \"tfidf_vectorizer.pkl\"))\n",
        "        joblib.dump(tfidf_matrix, os.path.join(out_dir, \"tfidf_matrix.pkl\"))\n",
        "        products_df.to_pickle(os.path.join(out_dir, \"products_df.pkl\"))\n",
        "        train_df.to_pickle(os.path.join(out_dir, \"train_ratings_df.pkl\"))\n",
        "        print(f\"\\nSaved artifacts under {out_dir}\")\n",
        "\n",
        "    # return main objects for interactive use\n",
        "    return {\n",
        "        'pred_matrix_df': pred_matrix_df,\n",
        "        'tfidf_matrix': tfidf_matrix,\n",
        "        'products_df': products_df,\n",
        "        'train_df': train_df,\n",
        "        'svd': svd,\n",
        "        'tfidf_vectorizer': tfidf_vectorizer\n",
        "    }\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    artifacts = train_and_demo(save_artifacts=True)\n",
        "\n",
        "    # Example: interactive usage\n",
        "    user_to_try = artifacts['train_df']['userId'].unique()[0]\n",
        "    print(\"\\nInteractive example for user:\", user_to_try)\n",
        "    recs = recommend_products(user_to_try, n=7,\n",
        "                              pred_matrix_df=artifacts['pred_matrix_df'],\n",
        "                              tfidf_matrix=artifacts['tfidf_matrix'],\n",
        "                              products_df=artifacts['products_df'],\n",
        "                              ratings_df=artifacts['train_df'],\n",
        "                              method='hybrid')\n",
        "    for r in recs:\n",
        "        print(f\"- {r['productId']} | {r['title']} | score={r['score']:.4f} -- reasons: {r['reasons']}\")"
      ]
    }
  ]
}